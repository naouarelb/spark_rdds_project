# ğŸš€ Spark Data Processing Project
This project is a Spark application designed to test how Spark work on large datasets in a distributed environment.
It runs inside a Docker-based Spark cluster (using Bitnami Spark images), making it easy to scale and reproduce.

# ğŸ“‚ Project Structure
```plaintext
.
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # Entry point of the Spark job
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ transformation/         # Transformation logic and helpers
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py         # Data analysis functions
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaning.py         # Data cleaning and preprocessing
â”‚   â”‚   â”‚   â”œâ”€â”€ utils.py            # General utilities (load, save, helpers)
â”‚   â”‚   â”‚   â””â”€â”€ logs.py             # Logging and monitoring helpers
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ output/                 # Job outputs (Parquet, CSV, etc.)
â”‚   â”‚   â”œâ”€â”€ README.md               # App-specific documentation (logic, transformations)
â”‚   â”‚   â””â”€â”€ .env                    # Environment variables for the app
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                       # Input datasets (CSV, JSON, etc.)
â”‚   â”œâ”€â”€ tmp/                        # Temporary working directory (intermediate files)
â”‚   â””â”€â”€ logs/                       # Spark/Job logs
â”‚
â”œâ”€â”€ docker-compose.yml              # Spark cluster setup (master + workers)
â”œâ”€â”€ Dockerfile                      # Custom Spark image definition
â”œâ”€â”€ generate_data                   # Script to generate mock/test data
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # Project-level documentation (setup, run instructions)
```

# âš™ï¸ Requirements

- Docker version 28.4.0
- docker-compose version 1.29.2
- Datasets in CSV format
- Python 3.10.12 